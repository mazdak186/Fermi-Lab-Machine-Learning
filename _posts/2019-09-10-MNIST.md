---
title: "MNIST"
date: 2020-04-15T15:34:30-04:00
categories:
  - School Project
tags:
  - Machine Learning
  - Python
toc: true
toc_label:
toc_icon: 'bars'
classes: wide

---

# Overview

The MNIST dataset is a set of handwritten digits that is commonly used as a benchmark for machine learning techniques. It is especially useful for building neural networks while spending minimal time preprocessing the images. The images are all centered with a fixed size of 28 by 28 pixels. This article will go through the steps necessary to build and train a neural network to identify handwritten digits outside of the given dataset. 

# Import Libraries

The Keras API is a user-friendly framework built on top of the Tensorflow platform. It is used to quickly develop deep-learning models by layering various building blocks. Numpy is a python library that will be used to handle the data we train the model with. And the four visualization libraries will help with understanding how the data is being manipulated.

```python
# import numpy for arrays
import numpy as np

# import keras libraries
import keras
from keras.models import load_model
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop
from keras.callbacks import EarlyStopping

# import visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from PIL import Image
```

<br/>
    
The MNIST dataset must first be imported into a training and testing datset. *train_data* is an array with a shape of (60000, 28, 28). This means it contains 60000 images with a 28 by 28 array of integers ranging from 0 to 255 where 0 is a pure white pixel and 255 is a pure black pixel. We can visualize the raw data of the first image below. 

```python
# import the data, split between train and test sets
(train_data, train_label), (test_data, test_label) = mnist.load_data()

# print the first datum
print(train_data[0])
```

    [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0]
     [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0]
     [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0]
     [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
     [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]
     
The pyplot library can help with visualizing the set of integers as an actual image of the handwritten digit.

```python
print("First X in the training data:")
plt.imshow(train_data[0], cmap='gray')
plt.show()
```
        
    First X in the training data:

![png](output_1_1.png)

train_label is an array with shape (60000). It contains the list of labels that correspond to each of the images in the train_data array. Having a labeled dataset means the algorithm doesn't have to come up with a list of classifications on its own. This is called supervised learning. 

```python
print("\nFirst label in the training data: " + str(train_label[0]))
```
    
    First label in the training data: 5
    
The test_data and test_label arrays will be used to test the trained model on a dataset outside of the data it used for learning. The testing dataset only has 10000 images as opposed to the 60000 images it was trained on.


The next step is to reshape the data arrays so that it only has two dimensions. This means each image will only be described by a single list of 784 pixels instead of 28 lists of 28 pixels. 
```python
train_data = train_data.reshape(60000, 784)
test_data = test_data.reshape(10000, 784)
print(train_data.shape, 'train samples')
print(test_data.shape, 'test samples')
```

    (60000, 784) train samples
    (10000, 784) test samples


Next the data must be converted from uint8 datatype to float32.

```python
# check datatype
print("original datatype: " + str(train_data.dtype))
# convert x values to float32
train_data = train_data.astype('float32')
test_data = test_data.astype('float32')
train_label = train_label.astype('float32')
test_label = test_label.astype('float32')
print("new datatype: " + str(train_data.dtype))
```

    original datatype: uint8
    new datatype: float32

Next the pixels must be scaled from 0 to 1

```python
print("mean raw pixel value: " + str(np.mean(train_data[0])))

# scale pixel values from 0 to 1
X_train /= 255
X_test /= 255

print("mean scaled pixel value: " + str(np.mean(train_data[0])))
```

    35.108418
    0.13768007
    
Next the label data must be converted to categortical data. In classification neural networks, the final output layer is an array of binary numbers that has the length equivalent to the number of possible classes being identified by the model. Only one of the numbers in the array can be a "1", which indicates the class the model categorized the input into. This dataset has 10 possible digits the model can identify. In the example below, the fifth index in the list of ten possible classes is flipped to "1" indicating that the label is the number five.

```python
# convert label data into categorical data
num_classes=10
train_label = keras.utils.to_categorical(train_label, num_classes)
test_label = keras.utils.to_categorical(test_label, num_classes)
print("\nFirst label in the training data is " + str(train_label[0]))
```

    
    First label in the training data is [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
    

To start with creating the model, there will be 3 dense layers. A dense layer is a regular deeply connected neural network layer. The first parameter in the dense layer is the number of units which is the output shape from a neuron. The activation parameter describes the function in which the algorithm will apply to the input of each neuron. The "relu" function is a non-linear function that returns the input if it is non-zero and returns zero if the input is negative.

```python
# get number of columns in sample data
ncols = train_data.shape[1]

# initalize sequentital model
model = Sequential()

# build the model with 3 dense layers

# Input layer has ncols (784) neurons
model.add(Dense(units=512, activation='relu', input_shape=(ncols,)))

# randomly drop 20% of neurons
model.add(Dropout(0.2))


model.add(Dense(512, activation='relu'))

model.add(Dropout(0.2))

# output layer to 10 neurons with softmax activation
model.add(Dense(num_classes, activation='softmax'))

model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_1 (Dense)              (None, 512)               401920    
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 512)               0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 512)               262656    
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 512)               0         
    _________________________________________________________________
    dense_3 (Dense)              (None, 10)                5130      
    =================================================================
    Total params: 669,706
    Trainable params: 669,706
    Non-trainable params: 0
    _________________________________________________________________
    


```python
# compile the model
model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(),
              metrics=['accuracy'])

# print the attributes of the model to ensure they're correct
print("Loss function: " + model.loss)
print("Optimizer: " + str(model.optimizer))
print("Metrics: " + str(model.metrics))
```

    Loss function: categorical_crossentropy
    Optimizer: <keras.optimizers.RMSprop object at 0x0000022CCFA47F28>
    Metrics: [<keras.metrics.MeanMetricWrapper object at 0x0000022CCE63AC18>]
    


```python
# choose the batch size
batch_size = 128

# initialize early stopping monitor
early_stopping_monitor = EarlyStopping(patience=3)

# fit the model 
fit1 = model.fit(X_train, y_train,
                    batch_size=batch_size,
                    epochs=100,
                    callbacks = [early_stopping_monitor],
                    verbose=1,
                    validation_data=(X_test, y_test))
```

    WARNING:tensorflow:From C:\Users\Maz\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.
    
    Train on 60000 samples, validate on 10000 samples
    Epoch 1/100
    60000/60000 [==============================] - 12s 196us/step - loss: 0.2470 - accuracy: 0.9238 - val_loss: 0.0955 - val_accuracy: 0.9707
    Epoch 2/100
    60000/60000 [==============================] - 11s 181us/step - loss: 0.1023 - accuracy: 0.9686 - val_loss: 0.1169 - val_accuracy: 0.9645
    Epoch 3/100
    60000/60000 [==============================] - 11s 177us/step - loss: 0.0777 - accuracy: 0.9764 - val_loss: 0.0851 - val_accuracy: 0.9761
    Epoch 4/100
    60000/60000 [==============================] - 10s 174us/step - loss: 0.0586 - accuracy: 0.9824 - val_loss: 0.0835 - val_accuracy: 0.9777
    Epoch 5/100
    60000/60000 [==============================] - 10s 171us/step - loss: 0.0514 - accuracy: 0.9848 - val_loss: 0.0716 - val_accuracy: 0.9811
    Epoch 6/100
    60000/60000 [==============================] - 11s 180us/step - loss: 0.0450 - accuracy: 0.9869 - val_loss: 0.0747 - val_accuracy: 0.9808
    Epoch 7/100
    60000/60000 [==============================] - 10s 170us/step - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.0793 - val_accuracy: 0.9810
    Epoch 8/100
    60000/60000 [==============================] - 11s 177us/step - loss: 0.0350 - accuracy: 0.9896 - val_loss: 0.0760 - val_accuracy: 0.9820
    


```python
# evaluate the model
score = model.evaluate(X_test, y_test, verbose=0)

# print model loss and accuracy
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```

    Test loss: 0.07599184588052357
    Test accuracy: 0.9819999933242798
    


```python
img = Image.open('one.png').convert("L")
plt.imshow(img, cmap='gray')
plt.show()
```


![png](output_11_0.png)



```python
#img = np.resize(img, (28,28))
img_ar = np.array(img)
img_re = img_ar.reshape(1,784)
img_fl = img_re.astype("float32")
img_sc = img_fl/255
y_pred = model.predict_classes(img_sc)
print(y_pred)
```

    [5]
    


```python
img_ar
```




    array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 255,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0]], dtype=uint8)




```python
for i in im2arr[0,:]:
    if (im2arr[0,i] < 200):
        im2arr[0,[i]] = 0
```
